{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset Classes with Kaggle Integration\n",
    "\n",
    "This notebook implements PyTorch dataset classes that automatically download datasets from Kaggle and provide proper `__getitem__` functionality for both detection and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub torch torchvision albumentations opencv-python pillow numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import kagglehub\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoundingBox Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    \"\"\"Represents a bounding box with coordinates and label.\"\"\"\n",
    "    x_min: float\n",
    "    y_min: float\n",
    "    x_max: float\n",
    "    y_max: float\n",
    "    label: str\n",
    "    confidence: float = 1.0\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary format.\"\"\"\n",
    "        return {\n",
    "            'x_min': self.x_min,\n",
    "            'y_min': self.y_min,\n",
    "            'x_max': self.x_max,\n",
    "            'y_max': self.y_max,\n",
    "            'label': self.label,\n",
    "            'confidence': self.confidence\n",
    "        }\n",
    "    \n",
    "    def area(self) -> float:\n",
    "        \"\"\"Calculate bounding box area.\"\"\"\n",
    "        return (self.x_max - self.x_min) * (self.y_max - self.y_min)\n",
    "    \n",
    "    def to_albumentations_format(self) -> List[float]:\n",
    "        \"\"\"Convert to albumentations format [x_min, y_min, x_max, y_max].\"\"\"\n",
    "        return [self.x_min, self.y_min, self.x_max, self.y_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDatasetManager:\n",
    "    \"\"\"Manages downloading and caching of Kaggle datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        \"\"\"Initialize the manager with optional cache directory.\"\"\"\n",
    "        if cache_dir is None:\n",
    "            self.cache_dir = Path(tempfile.gettempdir()) / \"kaggle_datasets_cache\"\n",
    "        else:\n",
    "            self.cache_dir = Path(cache_dir)\n",
    "        \n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Cache directory: {self.cache_dir}\")\n",
    "    \n",
    "    def download_dataset(self, dataset_handle: str, force_download: bool = False) -> Path:\n",
    "        \"\"\"Download a dataset from Kaggle and return the path.\"\"\"\n",
    "        dataset_name = dataset_handle.replace('/', '_')\n",
    "        cached_path = self.cache_dir / dataset_name\n",
    "        \n",
    "        if cached_path.exists() and not force_download:\n",
    "            print(f\"Using cached dataset: {cached_path}\")\n",
    "            return cached_path\n",
    "        \n",
    "        print(f\"Downloading dataset: {dataset_handle}\")\n",
    "        try:\n",
    "            # Download to temporary location first\n",
    "            temp_path = kagglehub.dataset_download(dataset_handle)\n",
    "            \n",
    "            # Move to our cache directory\n",
    "            if cached_path.exists():\n",
    "                shutil.rmtree(cached_path)\n",
    "            shutil.move(temp_path, cached_path)\n",
    "            \n",
    "            print(f\"Dataset downloaded and cached: {cached_path}\")\n",
    "            return cached_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading dataset {dataset_handle}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the entire cache directory.\"\"\"\n",
    "        if self.cache_dir.exists():\n",
    "            shutil.rmtree(self.cache_dir)\n",
    "            self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Cache cleared\")\n",
    "    \n",
    "    def list_cached_datasets(self) -> List[str]:\n",
    "        \"\"\"List all cached datasets.\"\"\"\n",
    "        if not self.cache_dir.exists():\n",
    "            return []\n",
    "        return [d.name for d in self.cache_dir.iterdir() if d.is_dir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionTransforms:\n",
    "    \"\"\"Transforms for object detection tasks with bounding boxes.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transforms(image_size: Tuple[int, int] = (416, 416)):\n",
    "        \"\"\"Get training transforms for detection.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.HueSaturationValue(p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            label_fields=['class_labels'],\n",
    "            min_visibility=0.3\n",
    "        ))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_val_transforms(image_size: Tuple[int, int] = (416, 416)):\n",
    "        \"\"\"Get validation transforms for detection.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            label_fields=['class_labels']\n",
    "        ))\n",
    "\n",
    "\n",
    "class ClassificationTransforms:\n",
    "    \"\"\"Transforms for classification tasks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_train_transforms(image_size: Tuple[int, int] = (224, 224)):\n",
    "        \"\"\"Get training transforms for classification.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.HueSaturationValue(p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_val_transforms(image_size: Tuple[int, int] = (224, 224)):\n",
    "        \"\"\"Get validation transforms for classification.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(height=image_size[0], width=image_size[1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AndrewMVDPyTorchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Andrew MVD face detection dataset with XML annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 kaggle_handle: str = \"andrewmvd/face-mask-detection\",\n",
    "                 split: str = \"train\",\n",
    "                 transform=None,\n",
    "                 train_ratio: float = 0.8,\n",
    "                 val_ratio: float = 0.1,\n",
    "                 test_ratio: float = 0.1,\n",
    "                 cache_dir: Optional[str] = None,\n",
    "                 force_download: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            kaggle_handle: Kaggle dataset handle\n",
    "            split: Dataset split ('train', 'val', 'test')\n",
    "            transform: Albumentations transform pipeline\n",
    "            train_ratio: Ratio for training split\n",
    "            val_ratio: Ratio for validation split\n",
    "            test_ratio: Ratio for test split\n",
    "            cache_dir: Directory to cache downloaded datasets\n",
    "            force_download: Force re-download even if cached\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Download dataset\n",
    "        self.manager = KaggleDatasetManager(cache_dir)\n",
    "        self.dataset_path = self.manager.download_dataset(kaggle_handle, force_download)\n",
    "        \n",
    "        # Find images and annotations\n",
    "        self.images_dir = self.dataset_path / \"images\"\n",
    "        self.annotations_dir = self.dataset_path / \"annotations\"\n",
    "        \n",
    "        if not self.images_dir.exists():\n",
    "            raise FileNotFoundError(f\"Images directory not found: {self.images_dir}\")\n",
    "        if not self.annotations_dir.exists():\n",
    "            raise FileNotFoundError(f\"Annotations directory not found: {self.annotations_dir}\")\n",
    "        \n",
    "        # Get all image files\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        all_images = [f for f in self.images_dir.iterdir() \n",
    "                     if f.suffix.lower() in image_extensions]\n",
    "        \n",
    "        # Filter images that have corresponding XML annotations\n",
    "        self.image_files = []\n",
    "        for img_file in all_images:\n",
    "            xml_file = self.annotations_dir / f\"{img_file.stem}.xml\"\n",
    "            if xml_file.exists():\n",
    "                self.image_files.append(img_file)\n",
    "        \n",
    "        if not self.image_files:\n",
    "            raise ValueError(\"No images with corresponding XML annotations found\")\n",
    "        \n",
    "        # Sort for consistent ordering\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        # Split dataset\n",
    "        total_samples = len(self.image_files)\n",
    "        train_end = int(total_samples * train_ratio)\n",
    "        val_end = train_end + int(total_samples * val_ratio)\n",
    "        \n",
    "        if split == \"train\":\n",
    "            self.image_files = self.image_files[:train_end]\n",
    "        elif split == \"val\":\n",
    "            self.image_files = self.image_files[train_end:val_end]\n",
    "        elif split == \"test\":\n",
    "            self.image_files = self.image_files[val_end:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}. Must be 'train', 'val', or 'test'\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.image_files)} images for {split} split\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.label_to_idx = {'with_mask': 0, 'without_mask': 1, 'mask_weared_incorrect': 2}\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def _parse_xml_annotation(self, xml_path: Path) -> List[BoundingBox]:\n",
    "        \"\"\"Parse XML annotation file and return list of bounding boxes.\"\"\"\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        bboxes = []\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            bbox_elem = obj.find('bndbox')\n",
    "            \n",
    "            x_min = float(bbox_elem.find('xmin').text)\n",
    "            y_min = float(bbox_elem.find('ymin').text)\n",
    "            x_max = float(bbox_elem.find('xmax').text)\n",
    "            y_max = float(bbox_elem.find('ymax').text)\n",
    "            \n",
    "            bbox = BoundingBox(\n",
    "                x_min=x_min,\n",
    "                y_min=y_min,\n",
    "                x_max=x_max,\n",
    "                y_max=y_max,\n",
    "                label=name\n",
    "            )\n",
    "            bboxes.append(bbox)\n",
    "        \n",
    "        return bboxes\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"Get a single data point.\"\"\"\n",
    "        if idx >= len(self.image_files):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset of size {len(self.image_files)}\")\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.image_files[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load annotations\n",
    "        xml_path = self.annotations_dir / f\"{img_path.stem}.xml\"\n",
    "        bboxes = self._parse_xml_annotation(xml_path)\n",
    "        \n",
    "        # Prepare data for transforms\n",
    "        if bboxes:\n",
    "            bbox_coords = [bbox.to_albumentations_format() for bbox in bboxes]\n",
    "            class_labels = [bbox.label for bbox in bboxes]\n",
    "        else:\n",
    "            bbox_coords = []\n",
    "            class_labels = []\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=bbox_coords,\n",
    "                class_labels=class_labels\n",
    "            )\n",
    "            image = transformed['image']\n",
    "            bbox_coords = transformed['bboxes']\n",
    "            class_labels = transformed['class_labels']\n",
    "        else:\n",
    "            # Convert to tensor if no transforms\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        # Convert labels to indices\n",
    "        class_indices = [self.label_to_idx.get(label, 0) for label in class_labels]\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'bboxes': torch.tensor(bbox_coords, dtype=torch.float32) if bbox_coords else torch.empty((0, 4)),\n",
    "            'labels': torch.tensor(class_indices, dtype=torch.long) if class_indices else torch.empty((0,), dtype=torch.long),\n",
    "            'image_path': str(img_path),\n",
    "            'image_id': idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face12kPyTorchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Face12k classification dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 kaggle_handle: str = \"ashishjangra27/face-mask-12k-images-dataset\",\n",
    "                 split: str = \"train\",\n",
    "                 transform=None,\n",
    "                 train_ratio: float = 0.8,\n",
    "                 val_ratio: float = 0.1,\n",
    "                 test_ratio: float = 0.1,\n",
    "                 cache_dir: Optional[str] = None,\n",
    "                 force_download: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            kaggle_handle: Kaggle dataset handle\n",
    "            split: Dataset split ('train', 'val', 'test')\n",
    "            transform: Albumentations transform pipeline\n",
    "            train_ratio: Ratio for training split\n",
    "            val_ratio: Ratio for validation split\n",
    "            test_ratio: Ratio for test split\n",
    "            cache_dir: Directory to cache downloaded datasets\n",
    "            force_download: Force re-download even if cached\n",
    "        \"\"\"\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Download dataset\n",
    "        self.manager = KaggleDatasetManager(cache_dir)\n",
    "        self.dataset_path = self.manager.download_dataset(kaggle_handle, force_download)\n",
    "        \n",
    "        # Find the main dataset directory\n",
    "        possible_dirs = ['Face Mask Dataset', 'dataset', 'data']\n",
    "        main_dir = None\n",
    "        \n",
    "        for dir_name in possible_dirs:\n",
    "            potential_path = self.dataset_path / dir_name\n",
    "            if potential_path.exists():\n",
    "                main_dir = potential_path\n",
    "                break\n",
    "        \n",
    "        if main_dir is None:\n",
    "            # Use the dataset path directly\n",
    "            main_dir = self.dataset_path\n",
    "        \n",
    "        # Look for class directories\n",
    "        class_dirs = [d for d in main_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        if not class_dirs:\n",
    "            raise FileNotFoundError(f\"No class directories found in {main_dir}\")\n",
    "        \n",
    "        # Collect all images with their labels\n",
    "        self.samples = []\n",
    "        self.classes = sorted([d.name for d in class_dirs])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        \n",
    "        for class_dir in class_dirs:\n",
    "            class_name = class_dir.name\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            for img_file in class_dir.iterdir():\n",
    "                if img_file.suffix.lower() in image_extensions:\n",
    "                    self.samples.append((img_file, class_idx, class_name))\n",
    "        \n",
    "        if not self.samples:\n",
    "            raise ValueError(\"No images found in the dataset\")\n",
    "        \n",
    "        # Sort for consistent ordering\n",
    "        self.samples.sort(key=lambda x: str(x[0]))\n",
    "        \n",
    "        # Split dataset\n",
    "        total_samples = len(self.samples)\n",
    "        train_end = int(total_samples * train_ratio)\n",
    "        val_end = train_end + int(total_samples * val_ratio)\n",
    "        \n",
    "        if split == \"train\":\n",
    "            self.samples = self.samples[:train_end]\n",
    "        elif split == \"val\":\n",
    "            self.samples = self.samples[train_end:val_end]\n",
    "        elif split == \"test\":\n",
    "            self.samples = self.samples[val_end:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {split}. Must be 'train', 'val', or 'test'\")\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} images for {split} split\")\n",
    "        print(f\"Classes: {self.classes}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"Get a single data point.\"\"\"\n",
    "        if idx >= len(self.samples):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset of size {len(self.samples)}\")\n",
    "        \n",
    "        img_path, class_idx, class_name = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        else:\n",
    "            # Convert to tensor if no transforms\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(class_idx, dtype=torch.long),\n",
    "            'class_name': class_name,\n",
    "            'image_path': str(img_path),\n",
    "            'image_id': idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Detection Dataset\n",
    "print(\"Testing Detection Dataset...\")\n",
    "try:\n",
    "    # Create transforms\n",
    "    train_transforms = DetectionTransforms.get_train_transforms()\n",
    "    val_transforms = DetectionTransforms.get_val_transforms()\n",
    "    \n",
    "    # Create dataset\n",
    "    detection_dataset = AndrewMVDPyTorchDataset(\n",
    "        split=\"train\",\n",
    "        transform=train_transforms,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"Detection dataset size: {len(detection_dataset)}\")\n",
    "    \n",
    "    # Test __getitem__\n",
    "    sample = detection_dataset[0]\n",
    "    print(f\"Sample keys: {sample.keys()}\")\n",
    "    print(f\"Image shape: {sample['image'].shape}\")\n",
    "    print(f\"Number of bboxes: {len(sample['bboxes'])}\")\n",
    "    print(f\"Number of labels: {len(sample['labels'])}\")\n",
    "    print(f\"Labels: {sample['labels']}\")\n",
    "    \n",
    "    print(\"Detection dataset test PASSED!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Detection dataset test FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Classification Dataset\n",
    "print(\"\\nTesting Classification Dataset...\")\n",
    "try:\n",
    "    # Create transforms\n",
    "    train_transforms = ClassificationTransforms.get_train_transforms()\n",
    "    val_transforms = ClassificationTransforms.get_val_transforms()\n",
    "    \n",
    "    # Create dataset\n",
    "    classification_dataset = Face12kPyTorchDataset(\n",
    "        split=\"train\",\n",
    "        transform=train_transforms,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"Classification dataset size: {len(classification_dataset)}\")\n",
    "    \n",
    "    # Test __getitem__\n",
    "    sample = classification_dataset[0]\n",
    "    print(f\"Sample keys: {sample.keys()}\")\n",
    "    print(f\"Image shape: {sample['image'].shape}\")\n",
    "    print(f\"Label: {sample['label']}\")\n",
    "    print(f\"Class name: {sample['class_name']}\")\n",
    "    \n",
    "    print(\"Classification dataset test PASSED!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Classification dataset test FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create DataLoader for training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Detection dataset with DataLoader\n",
    "detection_train_dataset = AndrewMVDPyTorchDataset(\n",
    "    split=\"train\",\n",
    "    transform=DetectionTransforms.get_train_transforms()\n",
    ")\n",
    "\n",
    "detection_train_loader = DataLoader(\n",
    "    detection_train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=lambda batch: {\n",
    "        'images': torch.stack([item['image'] for item in batch]),\n",
    "        'bboxes': [item['bboxes'] for item in batch],\n",
    "        'labels': [item['labels'] for item in batch],\n",
    "        'image_paths': [item['image_path'] for item in batch],\n",
    "        'image_ids': [item['image_id'] for item in batch]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Detection DataLoader created with {len(detection_train_loader)} batches\")\n",
    "\n",
    "# Classification dataset with DataLoader\n",
    "classification_train_dataset = Face12kPyTorchDataset(\n",
    "    split=\"train\",\n",
    "    transform=ClassificationTransforms.get_train_transforms()\n",
    ")\n",
    "\n",
    "classification_train_loader = DataLoader(\n",
    "    classification_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Classification DataLoader created with {len(classification_train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **KaggleDatasetManager**: Automatically downloads and caches Kaggle datasets to temporary directories\n",
    "2. **AndrewMVDPyTorchDataset**: PyTorch dataset class for object detection with XML annotations\n",
    "3. **Face12kPyTorchDataset**: PyTorch dataset class for image classification\n",
    "4. **Transform classes**: Albumentations-based data augmentation for both tasks\n",
    "5. **BoundingBox dataclass**: Structured representation for detection annotations\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Proper `__getitem__` methods that return formatted data points\n",
    "- ✅ Automatic Kaggle dataset downloading to temporary files\n",
    "- ✅ Intelligent caching system to avoid re-downloading\n",
    "- ✅ Train/validation/test splits with configurable ratios\n",
    "- ✅ Data augmentation with Albumentations\n",
    "- ✅ Compatible with PyTorch DataLoader\n",
    "- ✅ Self-contained implementation (no external imports from main codebase)\n",
    "\n",
    "### Usage:\n",
    "1. Run all cells in order\n",
    "2. The datasets will be automatically downloaded from Kaggle\n",
    "3. Use the dataset classes in your training loops\n",
    "4. Datasets are cached locally to avoid re-downloading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Face Mask Detection Dataset Loader\n",
    "\n",
    "This notebook contains all the necessary code to load and work with face mask detection datasets using PyTorch Dataset classes. It includes automatic Kaggle dataset downloading and caching functionality.\n",
    "\n",
    "## Features:\n",
    "- PyTorch Dataset classes with proper `__getitem__` implementation\n",
    "- Automatic Kaggle dataset downloading using kagglehub\n",
    "- Intelligent caching system\n",
    "- Data transforms for both detection and classification tasks\n",
    "- Support for multiple dataset formats\n",
    "- Train/validation/test split functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision kagglehub pillow opencv-python albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Dataset Manager\n",
    "\n",
    "This class handles automatic downloading and caching of Kaggle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDatasetManager:\n",
    "    \"\"\"Manages Kaggle dataset downloads and caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[str] = None):\n",
    "        if cache_dir is None:\n",
    "            cache_dir = os.path.join(tempfile.gettempdir(), 'kaggle_datasets')\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Cache file to track downloaded datasets\n",
    "        self.cache_file = self.cache_dir / 'dataset_cache.json'\n",
    "        self.cache_info = self._load_cache_info()\n",
    "    \n",
    "    def _load_cache_info(self) -> Dict[str, str]:\n",
    "        \"\"\"Load cache information from file.\"\"\"\n",
    "        if self.cache_file.exists():\n",
    "            try:\n",
    "                with open(self.cache_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError):\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def _save_cache_info(self):\n",
    "        \"\"\"Save cache information to file.\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'w') as f:\n",
    "                json.dump(self.cache_info, f, indent=2)\n",
    "        except IOError as e:\n",
    "            print(f\"Warning: Could not save cache info: {e}\")\n",
    "    \n",
    "    def download_dataset(self, dataset_id: str, force_download: bool = False) -> str:\n",
    "        \"\"\"Download a Kaggle dataset and return the local path.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: Kaggle dataset identifier (e.g., 'username/dataset-name')\n",
    "            force_download: If True, re-download even if cached\n",
    "            \n",
    "        Returns:\n",
    "            Local path to the downloaded dataset\n",
    "        \"\"\"\n",
    "        # Check if dataset is already cached\n",
    "        if not force_download and dataset_id in self.cache_info:\n",
    "            cached_path = self.cache_info[dataset_id]\n",
    "            if os.path.exists(cached_path):\n",
    "                print(f\"Using cached dataset: {cached_path}\")\n",
    "                return cached_path\n",
    "            else:\n",
    "                # Remove invalid cache entry\n",
    "                del self.cache_info[dataset_id]\n",
    "                self._save_cache_info()\n",
    "        \n",
    "        print(f\"Downloading dataset: {dataset_id}\")\n",
    "        try:\n",
    "            # Download using kagglehub\n",
    "            dataset_path = kagglehub.dataset_download(dataset_id)\n",
    "            \n",
    "            # Update cache info\n",
    "            self.cache_info[dataset_id] = dataset_path\n",
    "            self._save_cache_info()\n",
    "            \n",
    "            print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "            return dataset_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to download dataset {dataset_id}: {str(e)}\")\n",
    "    \n",
    "    def get_dataset_path(self, dataset_id: str) -> Optional[str]:\n",
    "        \"\"\"Get the local path of a dataset if it exists in cache.\"\"\"\n",
    "        if dataset_id in self.cache_info:\n",
    "            path = self.cache_info[dataset_id]\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "        return None\n",
    "    \n",
    "    def clear_cache(self, dataset_id: Optional[str] = None):\n",
    "        \"\"\"Clear cache for a specific dataset or all datasets.\"\"\"\n",
    "        if dataset_id:\n",
    "            if dataset_id in self.cache_info:\n",
    "                del self.cache_info[dataset_id]\n",
    "                print(f\"Cleared cache for dataset: {dataset_id}\")\n",
    "        else:\n",
    "            self.cache_info.clear()\n",
    "            print(\"Cleared all dataset cache\")\n",
    "        \n",
    "        self._save_cache_info()\n",
    "    \n",
    "    def list_cached_datasets(self) -> List[str]:\n",
    "        \"\"\"List all cached datasets.\"\"\"\n",
    "        valid_datasets = []\n",
    "        for dataset_id, path in self.cache_info.items():\n",
    "            if os.path.exists(path):\n",
    "                valid_datasets.append(dataset_id)\n",
    "        return valid_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures and Transforms\n",
    "\n",
    "Define data structures for bounding boxes and transform classes for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BoundingBox:\n",
    "    \"\"\"Represents a bounding box with label.\"\"\"\n",
    "    x_min: float\n",
    "    y_min: float\n",
    "    x_max: float\n",
    "    y_max: float\n",
    "    label: str\n",
    "    confidence: float = 1.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Union[float, str]]:\n",
    "        return {\n",
    "            'x_min': self.x_min,\n",
    "            'y_min': self.y_min,\n",
    "            'x_max': self.x_max,\n",
    "            'y_max': self.y_max,\n",
    "            'label': self.label,\n",
    "            'confidence': self.confidence\n",
    "        }\n",
    "    \n",
    "    def area(self) -> float:\n",
    "        return (self.x_max - self.x_min) * (self.y_max - self.y_min)\n",
    "    \n",
    "    def center(self) -> Tuple[float, float]:\n",
    "        return ((self.x_min + self.x_max) / 2, (self.y_min + self.y_max) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransforms:\n",
    "    \"\"\"Transform pipeline for classification tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 image_size: Tuple[int, int] = (224, 224),\n",
    "                 augment: bool = True,\n",
    "                 normalize: bool = True):\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Build transform pipeline\n",
    "        transform_list = []\n",
    "        \n",
    "        # Resize\n",
    "        transform_list.append(A.Resize(height=image_size[0], width=image_size[1]))\n",
    "        \n",
    "        # Augmentations for training\n",
    "        if augment:\n",
    "            transform_list.extend([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.3),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "                A.RandomGamma(p=0.2),\n",
    "                A.GaussNoise(p=0.2),\n",
    "                A.Blur(blur_limit=3, p=0.1)\n",
    "            ])\n",
    "        \n",
    "        # Normalization\n",
    "        if normalize:\n",
    "            transform_list.append(\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            )\n",
    "        \n",
    "        # Convert to tensor\n",
    "        transform_list.append(ToTensorV2())\n",
    "        \n",
    "        self.transform = A.Compose(transform_list)\n",
    "    \n",
    "    def __call__(self, image: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Apply transforms to image.\"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "        \n",
    "        # Ensure RGB format\n",
    "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        transformed = self.transform(image=image)\n",
    "        return transformed['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionTransforms:\n",
    "    \"\"\"Transform pipeline for detection tasks with bounding boxes.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 image_size: Tuple[int, int] = (416, 416),\n",
    "                 augment: bool = True,\n",
    "                 normalize: bool = True):\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Build transform pipeline\n",
    "        transform_list = []\n",
    "        \n",
    "        # Resize\n",
    "        transform_list.append(A.Resize(height=image_size[0], width=image_size[1]))\n",
    "        \n",
    "        # Augmentations for training\n",
    "        if augment:\n",
    "            transform_list.extend([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.3),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "                A.RandomGamma(p=0.2),\n",
    "                A.GaussNoise(p=0.2)\n",
    "            ])\n",
    "        \n",
    "        # Normalization\n",
    "        if normalize:\n",
    "            transform_list.append(\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            )\n",
    "        \n",
    "        # Convert to tensor\n",
    "        transform_list.append(ToTensorV2())\n",
    "        \n",
    "        # Compose with bbox support\n",
    "        self.transform = A.Compose(\n",
    "            transform_list,\n",
    "            bbox_params=A.BboxParams(\n",
    "                format='pascal_voc',\n",
    "                label_fields=['class_labels'],\n",
    "                min_visibility=0.3\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def __call__(self, image: np.ndarray, bboxes: List[BoundingBox]) -> Tuple[torch.Tensor, List[BoundingBox]]:\n",
    "        \"\"\"Apply transforms to image and bounding boxes.\"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = np.array(image)\n",
    "        \n",
    "        # Ensure RGB format\n",
    "        if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert bboxes to albumentations format\n",
    "        bbox_list = []\n",
    "        class_labels = []\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            bbox_list.append([bbox.x_min, bbox.y_min, bbox.x_max, bbox.y_max])\n",
    "            class_labels.append(bbox.label)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if bbox_list:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=bbox_list,\n",
    "                class_labels=class_labels\n",
    "            )\n",
    "            \n",
    "            # Convert back to BoundingBox objects\n",
    "            transformed_bboxes = []\n",
    "            for bbox_coords, label in zip(transformed['bboxes'], transformed['class_labels']):\n",
    "                transformed_bboxes.append(BoundingBox(\n",
    "                    x_min=bbox_coords[0],\n",
    "                    y_min=bbox_coords[1],\n",
    "                    x_max=bbox_coords[2],\n",
    "                    y_max=bbox_coords[3],\n",
    "                    label=label\n",
    "                ))\n",
    "            \n",
    "            return transformed['image'], transformed_bboxes\n",
    "        else:\n",
    "            # No bboxes, just transform image\n",
    "            transformed = self.transform(image=image, bboxes=[], class_labels=[])\n",
    "            return transformed['image'], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset Classes\n",
    "\n",
    "Implementation of PyTorch Dataset classes for different face mask detection datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AndrewMVDPyTorchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Andrew MVD Face Mask Detection dataset.\n",
    "    \n",
    "    This dataset contains images with XML annotations for face mask detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dataset_path: str,\n",
    "                 split: str = 'train',\n",
    "                 transform=None,\n",
    "                 task_type: str = 'detection',\n",
    "                 split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_path: Path to the dataset directory\n",
    "            split: 'train', 'val', or 'test'\n",
    "            transform: Transform to apply to images\n",
    "            task_type: 'detection' or 'classification'\n",
    "            split_ratios: (train, val, test) ratios\n",
    "        \"\"\"\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        # Find images and annotations\n",
    "        self.images_dir = self.dataset_path / 'images'\n",
    "        self.annotations_dir = self.dataset_path / 'annotations'\n",
    "        \n",
    "        if not self.images_dir.exists():\n",
    "            raise ValueError(f\"Images directory not found: {self.images_dir}\")\n",
    "        if not self.annotations_dir.exists():\n",
    "            raise ValueError(f\"Annotations directory not found: {self.annotations_dir}\")\n",
    "        \n",
    "        # Get all image files\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        all_images = []\n",
    "        for ext in image_extensions:\n",
    "            all_images.extend(list(self.images_dir.glob(f'*{ext}')))\n",
    "            all_images.extend(list(self.images_dir.glob(f'*{ext.upper()}')))\n",
    "        \n",
    "        # Filter images that have corresponding annotations\n",
    "        self.image_files = []\n",
    "        for img_path in all_images:\n",
    "            xml_path = self.annotations_dir / f\"{img_path.stem}.xml\"\n",
    "            if xml_path.exists():\n",
    "                self.image_files.append(img_path)\n",
    "        \n",
    "        if not self.image_files:\n",
    "            raise ValueError(\"No valid image-annotation pairs found\")\n",
    "        \n",
    "        # Sort for consistent ordering\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        # Split dataset\n",
    "        self._create_splits(split_ratios)\n",
    "        \n",
    "        # Label mapping for classification\n",
    "        self.label_to_idx = {\n",
    "            'with_mask': 0,\n",
    "            'without_mask': 1,\n",
    "            'mask_weared_incorrect': 2\n",
    "        }\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "    \n",
    "    def _create_splits(self, split_ratios: Tuple[float, float, float]):\n",
    "        \"\"\"Create train/val/test splits.\"\"\"\n",
    "        total_images = len(self.image_files)\n",
    "        train_ratio, val_ratio, test_ratio = split_ratios\n",
    "        \n",
    "        train_end = int(total_images * train_ratio)\n",
    "        val_end = int(total_images * (train_ratio + val_ratio))\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            self.image_files = self.image_files[:train_end]\n",
    "        elif self.split == 'val':\n",
    "            self.image_files = self.image_files[train_end:val_end]\n",
    "        elif self.split == 'test':\n",
    "            self.image_files = self.image_files[val_end:]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {self.split}\")\n",
    "    \n",
    "    def _parse_xml_annotation(self, xml_path: Path) -> List[BoundingBox]:\n",
    "        \"\"\"Parse XML annotation file.\"\"\"\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        bboxes = []\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            bbox_elem = obj.find('bndbox')\n",
    "            \n",
    "            x_min = float(bbox_elem.find('xmin').text)\n",
    "            y_min = float(bbox_elem.find('ymin').text)\n",
    "            x_max = float(bbox_elem.find('xmax').text)\n",
    "            y_max = float(bbox_elem.find('ymax').text)\n",
    "            \n",
    "            bboxes.append(BoundingBox(\n",
    "                x_min=x_min,\n",
    "                y_min=y_min,\n",
    "                x_max=x_max,\n",
    "                y_max=y_max,\n",
    "                label=name\n",
    "            ))\n",
    "        \n",
    "        return bboxes\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Get a single data point.\"\"\"\n",
    "        img_path = self.image_files[idx]\n",
    "        xml_path = self.annotations_dir / f\"{img_path.stem}.xml\"\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image_np = np.array(image)\n",
    "        \n",
    "        # Parse annotations\n",
    "        bboxes = self._parse_xml_annotation(xml_path)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            if self.task_type == 'detection':\n",
    "                image_tensor, transformed_bboxes = self.transform(image_np, bboxes)\n",
    "            else:\n",
    "                image_tensor = self.transform(image_np)\n",
    "                transformed_bboxes = bboxes\n",
    "        else:\n",
    "            # Convert to tensor without transforms\n",
    "            image_tensor = torch.from_numpy(image_np.transpose(2, 0, 1)).float() / 255.0\n",
    "            transformed_bboxes = bboxes\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            # For classification, use the most common label\n",
    "            if transformed_bboxes:\n",
    "                labels = [bbox.label for bbox in transformed_bboxes]\n",
    "                most_common_label = max(set(labels), key=labels.count)\n",
    "                label_idx = self.label_to_idx.get(most_common_label, 1)  # Default to without_mask\n",
    "            else:\n",
    "                label_idx = 1  # Default to without_mask if no annotations\n",
    "            \n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'label': label_idx,\n",
    "                'image_path': str(img_path)\n",
    "            }\n",
    "        else:\n",
    "            # For detection, return bboxes\n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'bboxes': transformed_bboxes,\n",
    "                'image_path': str(img_path)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face12kPyTorchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for Face Mask 12k Images dataset.\n",
    "    \n",
    "    This dataset is organized in folders by class (With Mask, Without Mask).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dataset_path: str,\n",
    "                 split: str = 'train',\n",
    "                 transform=None,\n",
    "                 split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_path: Path to the dataset directory\n",
    "            split: 'train', 'val', or 'test'\n",
    "            transform: Transform to apply to images\n",
    "            split_ratios: (train, val, test) ratios\n",
    "        \"\"\"\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Find class directories\n",
    "        self.class_dirs = []\n",
    "        for item in self.dataset_path.iterdir():\n",
    "            if item.is_dir() and not item.name.startswith('.'):\n",
    "                self.class_dirs.append(item)\n",
    "        \n",
    "        if not self.class_dirs:\n",
    "            raise ValueError(f\"No class directories found in {self.dataset_path}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.class_dirs.sort()  # Ensure consistent ordering\n",
    "        self.label_to_idx = {cls_dir.name: idx for idx, cls_dir in enumerate(self.class_dirs)}\n",
    "        self.idx_to_label = {v: k for k, v in self.label_to_idx.items()}\n",
    "        \n",
    "        # Collect all image files\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        for class_dir in self.class_dirs:\n",
    "            class_label = self.label_to_idx[class_dir.name]\n",
    "            \n",
    "            for ext in image_extensions:\n",
    "                for img_path in class_dir.glob(f'*{ext}'):\n",
    "                    self.image_files.append(img_path)\n",
    "                    self.labels.append(class_label)\n",
    "                for img_path in class_dir.glob(f'*{ext.upper()}'):\n",
    "                    self.image_files.append(img_path)\n",
    "                    self.labels.append(class_label)\n

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detection Dataset Testing Notebook\n",
    "\n",
    "This notebook provides comprehensive testing of the PyTorch dataset classes for face mask detection.\n",
    "\n",
    "## Features:\n",
    "- **Self-contained**: All required code is embedded\n",
    "- **Kaggle integration**: Automatic dataset downloading\n",
    "- **Multiple datasets**: Support for 3 Kaggle datasets\n",
    "- **Comprehensive testing**: Validation and visualization\n",
    "\n",
    "## Supported Datasets:\n",
    "1. **andrewmvd/face-mask-detection** - Detection + classification\n",
    "2. **ashishjangra27/face-mask-12k-images-dataset** - Classification\n",
    "3. **wobotintelligence/face-mask-detection-dataset** - Medical detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision kagglehub Pillow tqdm matplotlib opencv-python pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Implementation\n",
    "\n",
    "All required dataset classes embedded directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDED DATASET IMPLEMENTATION - ALL REQUIRED CODE\n",
    "# ============================================================================\n",
    "\n",
    "# Additional imports for embedded implementation\n",
    "import hashlib\n",
    "import tempfile\n",
    "import pickle\n",
    "\n",
    "class KaggleDatasetManager:\n",
    "    \"\"\"Manages Kaggle dataset downloads and caching\"\"\"\n",
    "    \n",
    "    # Supported datasets with their metadata\n",
    "    SUPPORTED_DATASETS = {\n",
    "        'andrewmvd/face-mask-detection': {\n",
    "            'name': 'andrewmvd_face_mask',\n",
    "            'type': 'detection',\n",
    "            'expected_folders': ['images', 'annotations'],\n",
    "            'description': 'Face mask detection with bounding boxes'\n",
    "        },\n",
    "        'ashishjangra27/face-mask-12k-images-dataset': {\n",
    "            'name': 'face_mask_12k',\n",
    "            'type': 'classification',\n",
    "            'expected_folders': ['without_mask', 'with_mask', 'mask_weared_incorrect'],\n",
    "            'description': '12k face mask classification images'\n",
    "        },\n",
    "        'wobotintelligence/face-mask-detection-dataset': {\n",
    "            'name': 'medical_mask',\n",
    "            'type': 'detection',\n",
    "            'expected_folders': ['images', 'annotations'],\n",
    "            'description': 'Medical mask detection dataset'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"/tmp/face_mask_datasets\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.metadata_file = self.cache_dir / \"dataset_metadata.json\"\n",
    "        self.metadata = self._load_metadata()\n",
    "    \n",
    "    def _load_metadata(self) -> Dict[str, Any]:\n",
    "        if self.metadata_file.exists():\n",
    "            try:\n",
    "                with open(self.metadata_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError):\n",
    "                pass\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        try:\n",
    "            with open(self.metadata_file, 'w') as f:\n",
    "                json.dump(self.metadata, f, indent=2)\n",
    "        except IOError:\n",
    "            print(f\"Warning: Could not save metadata to {self.metadata_file}\")\n",
    "    \n",
    "    def _get_dataset_cache_path(self, dataset_id: str) -> Path:\n",
    "        if dataset_id not in self.SUPPORTED_DATASETS:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_id}\")\n",
    "        dataset_name = self.SUPPORTED_DATASETS[dataset_id]['name']\n",
    "        return self.cache_dir / dataset_name\n",
    "    \n",
    "    def _verify_dataset_structure(self, dataset_path: Path, dataset_id: str) -> bool:\n",
    "        if not dataset_path.exists():\n",
    "            return False\n",
    "        expected_folders = self.SUPPORTED_DATASETS[dataset_id]['expected_folders']\n",
    "        for folder in expected_folders:\n",
    "            folder_path = dataset_path / folder\n",
    "            if not folder_path.exists() or not folder_path.is_dir():\n",
    "                return False\n",
    "            if not any(folder_path.iterdir()):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def is_dataset_cached(self, dataset_id: str) -> bool:\n",
    "        if dataset_id not in self.SUPPORTED_DATASETS:\n",
    "            return False\n",
    "        cache_path = self._get_dataset_cache_path(dataset_id)\n",
    "        if not self._verify_dataset_structure(cache_path, dataset_id):\n",
    "            return False\n",
    "        if dataset_id in self.metadata:\n",
    "            metadata = self.metadata[dataset_id]\n",
    "            if metadata.get('status') == 'complete' and metadata.get('path') == str(cache_path):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_cached_dataset_path(self, dataset_id: str) -> Optional[str]:\n",
    "        if self.is_dataset_cached(dataset_id):\n",
    "            return str(self._get_dataset_cache_path(dataset_id))\n",
    "        return None\n",
    "    \n",
    "    def download_dataset(self, dataset_id: str, force_redownload: bool = False) -> str:\n",
    "        if dataset_id not in self.SUPPORTED_DATASETS:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_id}. \"\n",
    "                           f\"Supported: {list(self.SUPPORTED_DATASETS.keys())}\")\n",
    "        \n",
    "        if not force_redownload and self.is_dataset_cached(dataset_id):\n",
    "            cached_path = self.get_cached_dataset_path(dataset_id)\n",
    "            print(f\"Using cached dataset: {cached_path}\")\n",
    "            return cached_path\n",
    "        \n",
    "        try:\n",
    "            import kagglehub\n",
    "        except ImportError:\n",
    "            raise ImportError(\"kagglehub is required for dataset downloading. \"\n",
    "                            \"Install with: pip install kagglehub\")\n",
    "        \n",
    "        print(f\"Downloading dataset: {dataset_id}\")\n",
    "        \n",
    "        try:\n",
    "            temp_path = kagglehub.dataset_download(dataset_id)\n",
    "            temp_path = Path(temp_path)\n",
    "            \n",
    "            if not self._verify_dataset_structure(temp_path, dataset_id):\n",
    "                raise RuntimeError(f\"Downloaded dataset has invalid structure: {temp_path}\")\n",
    "            \n",
    "            cache_path = self._get_dataset_cache_path(dataset_id)\n",
    "            \n",
    "            if cache_path.exists():\n",
    "                shutil.rmtree(cache_path)\n",
    "            \n",
    "            shutil.move(str(temp_path), str(cache_path))\n",
    "            \n",
    "            self.metadata[dataset_id] = {\n",
    "                'path': str(cache_path),\n",
    "                'download_time': time.time(),\n",
    "                'status': 'complete',\n",
    "                'dataset_info': self.SUPPORTED_DATASETS[dataset_id]\n",
    "            }\n",
    "            self._save_metadata()\n",
    "            \n",
    "            print(f\"Dataset cached at: {cache_path}\")\n",
    "            return str(cache_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to download dataset {dataset_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "class DatasetCacheManager:\n",
    "    \"\"\"Manages caching of processed dataset samples\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"/tmp/face_mask_cache\", max_cache_size_gb: float = 5.0):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.max_cache_size = max_cache_size_gb * 1024 * 1024 * 1024\n",
    "        self.metadata_file = self.cache_dir / \"cache_metadata.json\"\n",
    "        self.metadata = self._load_metadata()\n",
    "    \n",
    "    def _load_metadata(self) -> Dict[str, Any]:\n",
    "        if self.metadata_file.exists():\n",
    "            try:\n",
    "                with open(self.metadata_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except (json.JSONDecodeError, IOError):\n",
    "                pass\n",
    "        return {'datasets': {}, 'total_size': 0, 'last_cleanup': time.time()}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        try:\n",
    "            with open(self.metadata_file, 'w') as f:\n",
    "                json.dump(self.metadata, f, indent=2)\n",
    "        except IOError:\n",
    "            print(f\"Warning: Could not save cache metadata to {self.metadata_file}\")\n",
    "    \n",
    "    def _get_cache_key(self, dataset_id: str, split: str, mode: str, transform_hash: str) -> str:\n",
    "        key_string = f\"{dataset_id}_{split}_{mode}_{transform_hash}\"\n",
    "        return hashlib.md5(key_string.encode()).hexdigest()\n",
    "    \n",
    "    def _get_transform_hash(self, transforms) -> str:\n",
    "        if transforms is None:\n",
    "            return \"no_transforms\"\n",
    "        transform_str = str(transforms.__class__.__name__)\n",
    "        if hasattr(transforms, 'image_size'):\n",
    "            transform_str += f\"_size{transforms.image_size}\"\n",
    "        if hasattr(transforms, 'augment'):\n",
    "            transform_str += f\"_aug{transforms.augment}\"\n",
    "        return hashlib.md5(transform_str.encode()).hexdigest()[:8]\n",
    "    \n",
    "    def _get_cache_path(self, cache_key: str) -> Path:\n",
    "        return self.cache_dir / f\"{cache_key}.pkl\"\n",
    "    \n",
    "    def is_cached(self, dataset_id: str, split: str, mode: str, transforms) -> bool:\n",
    "        transform_hash = self._get_transform_hash(transforms)\n",
    "        cache_key = self._get_cache_key(dataset_id, split, mode, transform_hash)\n",
    "        cache_path = self._get_cache_path(cache_key)\n",
    "        return cache_path.exists()\n",
    "    \n",
    "    def save_to_cache(self, dataset_id: str, split: str, mode: str, transforms, samples: List[Tuple[Any, Any]]):\n",
    "        transform_hash = self._get_transform_hash(transforms)\n",
    "        cache_key = self._get_cache_key(dataset_id, split, mode, transform_hash)\n",
    "        cache_path = self._get_cache_path(cache_key)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(samples, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f\"Cached {len(samples)} samples for {dataset_id} ({split}, {mode})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to cache samples: {e}\")\n",
    "            if cache_path.exists():\n",
    "                cache_path.unlink()\n",
    "    \n",
    "    def load_from_cache(self, dataset_id: str, split: str, mode: str, transforms) -> Optional[List[Tuple[Any, Any]]]:\n",
    "        transform_hash = self._get_transform_hash(transforms)\n",
    "        cache_key = self._get_cache_key(dataset_id, split, mode, transform_hash)\n",
    "        cache_path = self._get_cache_path(cache_key)\n",
    "        \n",
    "        if not cache_path.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                samples = pickle.load(f)\n",
    "            cache_path.touch()\n",
    "            print(f\"Loaded {len(samples)} cached samples for {dataset_id} ({split}, {mode})\")\n",
    "            return samples\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cached samples: {e}\")\n",
    "            try:\n",
    "                cache_path.unlink()\n",
    "            except OSError:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "\n",
    "class DetectionTransforms:\n",
    "    \"\"\"Transforms for object detection tasks that handle both images and bounding boxes\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size: int = 416, augment: bool = True):\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __call__(self, image: Union[Image.Image, np.ndarray], target: Dict[str, Any]) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.dtype == np.uint8:\n",
    "                image = Image.fromarray(image)\n",
    "            else:\n",
    "                image = Image.fromarray((image * 255).astype(np.uint8))\n",
    "        \n",
    "        orig_w, orig_h = image.size\n",
    "        \n",
    "        boxes = target['boxes']\n",
    "        if not isinstance(boxes, torch.Tensor):\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        labels = target['labels']\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                image = F.hflip(image)\n",
    "                boxes[:, [0, 2]] = orig_w - boxes[:, [2, 0]]\n",
    "            \n",
    "            if random.random() < 0.5:\n",
    "                color_jitter = T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "                image = color_jitter(image)\n",
    "        \n",
    "        image = F.resize(image, (self.image_size, self.image_size))\n",
    "        \n",
    "        scale_x = self.image_size / orig_w\n",
    "        scale_y = self.image_size / orig_h\n",
    "        \n",
    "        boxes[:, [0, 2]] *= scale_x\n",
    "        boxes[:, [1, 3]] *= scale_y\n",
    "        \n",
    "        image = F.to_tensor(image)\n",
    "        image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': target.get('image_id', 0),\n",
    "            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
    "        }\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ClassificationTransforms:\n",
    "    \"\"\"Transforms for classification tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size: int = 224, augment: bool = True):\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.base_transforms = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if augment:\n",
    "            self.augment_transforms = T.Compose([\n",
    "                T.Resize((int(image_size * 1.1), int(image_size * 1.1))),\n",
    "                T.RandomCrop((image_size, image_size)),\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                T.RandomRotation(degrees=10),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.augment_transforms = self.base_transforms\n",
    "    \n",
    "    def __call__(self, image: Union[Image.Image, np.ndarray]) -> torch.Tensor:\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "                if image.max() <= 1.0:\n",
    "                    image = (image * 255).astype(np.uint8)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = Image.fromarray(image)\n",
    "            else:\n",
    "                image = Image.fromarray(image)\n",
    "        \n",
    "        if self.augment:\n",
    "            return self.augment_transforms(image)\n",
    "        else:\n",
    "            return self.base_transforms(image)\n",
    "\n",
    "\n",
    "class FaceMaskTransforms:\n",
    "    \"\"\"Factory class for creating face mask detection transforms\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_detection_transforms(image_size: int = 416, augment: bool = True) -> DetectionTransforms:\n",
    "        return DetectionTransforms(image_size=image_size, augment=augment)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_classification_transforms(image_size: int = 224, augment: bool = True) -> ClassificationTransforms:\n",
    "        return ClassificationTransforms(image_size=image_size, augment=augment)\n",
    "\n",
    "\n",
    "class AndrewMVDPyTorchDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for andrewmvd/face-mask-detection from Kaggle\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 kaggle_dataset_id: str = 'andrewmvd/face-mask-detection',\n",
    "                 mode: str = 'detection',\n",
    "                 split: str = 'train',\n",
    "                 split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15),\n",
    "                 transforms=None,\n",
    "                 cache_processed: bool = True,\n",
    "                 cache_dir: str = \"/tmp/face_mask_datasets\",\n",
    "                 download_to_tmp: bool = True,\n",
    "                 random_seed: int = 42):\n",
    "        \n",
    "        self.kaggle_dataset_id = kaggle_dataset_id\n",
    "        self.mode = mode.lower()\n",
    "        self.split = split.lower()\n",
    "        self.split_ratios = split_ratios\n",
    "        self.transforms = transforms\n",
    "        self.cache_processed = cache_processed\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self.kaggle_manager = KaggleDatasetManager(cache_dir)\n",
    "        if cache_processed:\n",
    "            self.cache_manager = DatasetCacheManager()\n",
    "        \n",
    "        print(f\"Initializing AndrewMVD dataset (mode: {mode}, split: {split})\")\n",
    "        self.dataset_path = self.kaggle_manager.download_dataset(kaggle_dataset_id)\n",
    "        \n",
    "        self.images_path = os.path.join(self.dataset_path, 'images')\n",
    "        self.annotations_path = os.path.join(self.dataset_path, 'annotations')\n",
    "        \n",
    "        self._load_samples()\n",
    "        \n",
    "        if split != 'all':\n",
    "            self._apply_split()\n",
    "        \n",
    "        if transforms is None:\n",
    "            if mode == 'detection':\n",
    "                self.transforms = FaceMaskTransforms.get_detection_transforms(augment=(split == 'train'))\n",
    "            else:\n",
    "                self.transforms = FaceMaskTransforms.get_classification_transforms(augment=(split == 'train'))\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self)} samples\")\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        if self.cache_processed and hasattr(self, 'cache_manager'):\n",
    "            cached_samples = self.cache_manager.load_from_cache(\n",
    "                self.kaggle_dataset_id, 'all', self.mode, None\n",
    "            )\n",
    "            if cached_samples is not None:\n",
    "                self.samples = cached_samples\n",
    "                return\n",
    "        \n",
    "        print(\"Processing dataset samples...\")\n",
    "        self.samples = []\n",
    "        self.classes = set()\n",
    "        \n",
    "        annotation_files = [f for f in os.listdir(self.annotations_path) if f.endswith('.xml')]\n",
    "        \n",
    "        for anno_file in annotation_files:\n",
    "            image_id = os.path.splitext(anno_file)[0]\n",
    "            \n",
    "            img_path = None\n",
    "            for ext in ['.png', '.jpg', '.jpeg']:\n",
    "                potential_path = os.path.join(self.images_path, f\"{image_id}{ext}\")\n",
    "                if os.path.exists(potential_path):\n",
    "                    img_path = potential_path\n",
    "                    break\n",
    "            \n",
    "            if img_path is None:\n",
    "                continue\n",
    "            \n",
    "            anno_path = os.path.join(self.annotations_path, anno_file)\n",
    "            try:\n",
    "                tree = ET.parse(anno_path)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                boxes = []\n",
    "                labels = []\n",
    "                \n",
    "                for obj in root.findall('object'):\n",
    "                    class_name = obj.find('name').text\n",
    "                    self.classes.add(class_name)\n",
    "                    \n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(float(bndbox.find('xmin').text))\n",
    "                    ymin = int(float(bndbox.find('ymin').text))\n",
    "                    xmax = int(float(bndbox.find('xmax').text))\n",
    "                    ymax = int(float(bndbox.find('ymax').text))\n",
    "                    \n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(class_name)\n",
    "                \n",
    "                if boxes:\n",
    "                    self.samples.append({\n",
    "                        'image_path': img_path,\n",
    "                        'boxes': boxes,\n",
    "                        'labels': labels,\n",
    "                        'image_id': len(self.samples)\n",
    "                    })\n",
    "                    \n",
    "            except ET.ParseError:\n",
    "                print(f\"Warning: Could not parse annotation {anno_file}\")\n",
    "                continue\n",
    "        \n",
    "        sorted_classes = sorted(list(self.classes))
        self.class_mapping = {name: idx for idx, name in enumerate(sorted_classes)}
        self.class_names = sorted_classes
        
        if self.mode == 'classification':
            self._create_cropped_samples()
        
        if self.cache_processed and hasattr(self, 'cache_manager'):
            self.cache_manager.save_to_cache(
                self.kaggle_dataset_id, 'all', self.mode, None, self.samples
            )
    
    def _create_cropped_samples(self):
        print("Creating cropped face samples...")
        cropped_samples = []
        
        for sample in self.samples:
            try:
                image = cv2.imread(sample['image_path'])
                if image is None:
                    continue
                
                for box, label in zip(sample['boxes'], sample['labels']):
                    xmin, ymin, xmax, ymax = box
                    
                    h, w = image.shape[:2]
                    xmin = max(0, min(xmin, w-1))
                    xmax = max(0, min(xmax, w))
                    ymin = max(0, min(ymin, h-1))
                    ymax = max(0, min(ymax, h))
                    
                    if xmax > xmin and ymax > ymin:
                        crop = image[ymin:ymax, xmin:xmax]
                        cropped_samples.append({
                            'crop': crop,
                            'label': self.class_mapping[label],
                            'original_image_id': sample['image_id']
                        })
            except Exception as e:
                print(f"Warning: Could not process image {sample['image_path']}: {e}")
                continue
        
        self.samples = cropped_samples
        print(f"Created {len(cropped_samples)} cropped face samples")
    
    def _apply_split(self):
        total_samples = len(self.samples)
        random.shuffle(self.samples)
        
        train_size = int(self.split_ratios[0] * total_samples)
        val_size = int(self.split_ratios[1] * total_samples)
        
        if self.split == 'train':
            self.samples = self.samples[:train_size]
        elif self.split == 'val':
            self.samples = self.samples[train_size:train_size + val_size]
        elif self.split == 'test':
            self.samples = self.samples[train_size + val_size:]
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Any]:
        sample = self.samples[idx]
        
        if self.mode == 'detection':
            image = cv2.imread(sample['image_path'])
            if image is None:
                image = np.zeros((416, 416, 3), dtype=np.uint8)
            
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
            
            boxes = torch.tensor(sample['boxes'], dtype=torch.float32)
            labels = torch.tensor([self.class_mapping[label] for label in sample['labels']], dtype=torch.int64)
            
            target = {
                'boxes': boxes,
                'labels': labels,
                'image_id': sample['image_id']
            }
            
            if self.transforms:
                image, target = self.transforms(image, target)
            
            return image, target
        
        else:  # classification mode
            crop = sample['crop']
            crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            crop = Image.fromarray(crop)
            
            label = sample['label']
            
            if self.transforms:
                crop = self.transforms(crop)
            
            return crop, label
    
    def get_class_names(self) -> List[str]:
        return self.class_names.copy()
    
    def get_dataset_info(self) -> Dict[str, Any]:
        return {
            'dataset_id': self.kaggle_dataset_id,
            'mode': self.mode,
            'split': self.split,
            'num_samples': len(self),
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'split_ratios': self.split_ratios
        }


class Face12kPyTorchDataset(Dataset):
    \"\"\"PyTorch Dataset for ashishjangra27/face-mask-12k-images-dataset from Kaggle\"\"\"
    
    def __init__(self,
                 kaggle_dataset_id: str = 'ashishjangra27/face-mask-12k-images-dataset',
                 split: str = 'train',
                 split_ratios: Tuple[float, float, float] = (0.7, 0.15, 0.15),
                 transforms=None,
                 cache_processed: bool = True,
                 cache_dir: str = "/tmp/face_mask_datasets",
                 download_to_tmp: bool = True,
                 random_seed: int = 42):
        
        self.kaggle_dataset_id = kaggle_dataset_id
        self.split = split.lower()
        self.split_ratios = split_ratios
        self.transforms = transforms
        self.cache_processed = cache_processed
        self.random_seed = random_seed
        
        random.seed(random_seed)
        np.random.seed(random_seed)
        
        self.kaggle_manager = KaggleDatasetManager(cache_dir)
        if cache_processed:
            self.cache_manager = DatasetCacheManager()
        
        print(f"Initializing Face12k dataset (split: {split})")
        self.dataset_path = self.kaggle_manager.download_dataset(kaggle_dataset_id)
        
        self.class_folders = {
            'without_mask': 0,
            'with_mask': 1,
            'mask_weared_incorrect': 2
        }
        self.class_names = ['without_mask', 'with_mask', 'mask_weared_incorrect']
        
        self._load_samples()
        
        if split != 'all':
            self._apply_split()
        
        if transforms is None:
            self.transforms = FaceMaskTransforms.get_classification_transforms(augment=(split == 'train'))
        
        print(f"Dataset initialized with {len(self)} samples")
    
    def _load_samples(self):
        if self.cache_processed and hasattr(self, 'cache_manager'):
            cached_samples = self.cache_manager.load_from_cache(
                self.kaggle_dataset_id, 'all', 'classification', None
            )
            if cached_samples is not None:
                self.samples = cached_samples
                return
        
        print("Processing Face12k samples...")
        self.samples = []
        
        for class_name, class_idx in self.class_folders.items():
            class_path = os.path.join(self.dataset_path, class_name)
            
            if not os.path.exists(class_path):
                print(f"Warning: Class folder {class_path} not found")
                continue
            
            for img_file in os.listdir(class_path):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(class_path, img_file)
                    self.samples.append({
                        'image_path': img_path,
                        'label': class_idx,
                        'class_name': class_name
                    })
        
        random.shuffle(self.samples)
        
        if self.cache_processed and hasattr(self, 'cache_manager'):
            self.cache_manager.save_to_cache(
                self.kaggle_dataset_id, 'all', 'classification', None, self.samples
            )
    
    def _apply_split(self):
        total_samples = len(self.samples)
        
        train_size = int(self.split_ratios[0] * total_samples)
        val_size = int(self.split_ratios[1] * total_samples)
        
        if self.split == 'train':
            self.samples = self.samples[:train_size]
        elif self.split == 'val':
            self.samples = self.samples[train_size:train_size + val_size]
        elif self.split == 'test':
            self.samples = self.samples[train_size + val_size:]
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        sample = self.samples[idx]
        
        try:
            image = cv2.imread(sample['image_path'])
            if image is None:
                image = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            image = Image.fromarray(image)
        except Exception as e:
            print(f"Warning: Could not load image {sample['image_path']}: {e}")
            image = Image.new('RGB', (224, 224), color=(0, 0, 0))
        
        label = sample['label']
        
        if self.transforms:
            image = self.transforms(image)
        
        return image, label
    
    def get_class_names(self) -> List[str]:
        return self.class_names.copy()
    
    def get_dataset_info(self) -> Dict[str, Any]:
        return {
            'dataset_id': self.kaggle_dataset_id,
            'mode': 'classification',
            'split': self.split,
            'num_samples': len(self),
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'split_ratios': self.split_ratios
        }


class DatasetFactory:
    \"\"\"Factory class for creating face mask detection datasets\"\"\"
    
    SUPPORTED_DATASETS = {
        'andrewmvd': AndrewMVDPyTorchDataset,
        'face12k': Face12kPyTorchDataset
    }
    
    @staticmethod
    def create_dataset(dataset_type: str, **kwargs) -> Dataset:
        dataset_type = dataset_type.lower()
        
        if dataset_type not in DatasetFactory.SUPPORTED_DATASETS:
            raise ValueError(f"Unsupported dataset type: {dataset_type}. "
                           f"Supported: {list(DatasetFactory.SUPPORTED_DATASETS.keys())}")
        
        dataset_class = DatasetFactory.SUPPORTED_DATASETS[dataset_type]
        return dataset_class(**kwargs)
    
    @staticmethod
    def list_supported_datasets() -> List[str]:
        return list(DatasetFactory.SUPPORTED_DATASETS.keys())


print("âœ“ All dataset classes loaded successfully!")
